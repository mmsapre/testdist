package com.acme.graavl.audit;

import com.acme.graavl.model.DistributionAuditEvent;
import com.hazelcast.core.HazelcastException;
import com.hazelcast.function.FunctionEx;
import com.hazelcast.ringbuffer.ReadResultSet;
import com.hazelcast.ringbuffer.Ringbuffer;
import org.springframework.r2dbc.core.DatabaseClient;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.util.concurrent.atomic.AtomicLong;

public class AuditRingBufferR2dbcWriter {

    private final Ringbuffer<DistributionAuditEvent> rb;
    private final DatabaseClient db;
    private final String table;

    /** Next sequence to read */
    private final AtomicLong seq = new AtomicLong();

    public AuditRingBufferR2dbcWriter(
            Ringbuffer<DistributionAuditEvent> rb,
            DatabaseClient db,
            String table) {

        this.rb = rb;
        this.db = db;
        this.table = table;
    }

    /**
     * Starts periodic draining.
     * On startup, we begin from the current tail + 1
     * (change to headSequence() if you want replay).
     */
    public Mono<Void> start() {

        return Mono.fromRunnable(() ->
                    seq.set(rb.tailSequence() + 1)
                )
                .thenMany(
                        Flux.interval(Duration.ofMillis(200))
                            .concatMap(tick -> drainOnce(2000))
                )
                .then();
    }

    private Mono<Void> drainOnce(int maxCount) {

        long startSeq = seq.get();
        long tail = rb.tailSequence();

        if (tail < startSeq) {
            return Mono.empty();
        }

        int toRead = (int) Math.min(maxCount, tail - startSeq + 1);

        return Mono.fromFuture(
                    rb.readManyAsync(
                            startSeq,
                            1,              // minCount
                            toRead,         // maxCount
                            (FunctionEx<DistributionAuditEvent, Boolean>) null
                    ).toCompletableFuture()
                )
                .flatMap(rs -> writeBatch(rs, startSeq));
    }

    private Mono<Void> writeBatch(ReadResultSet<DistributionAuditEvent> rs,
                                  long startSeq) {

        if (rs.isEmpty()) {
            return Mono.empty();
        }

        return Flux.fromIterable(rs)
                .concatMap(this::insertOne)
                .then(Mono.fromRunnable(() -> {
                    // Advance sequence correctly using actual committed sequence numbers
                    long lastSeq = rs.getSequence(rs.size() - 1);
                    seq.set(lastSeq + 1);
                }));
    }

    private Mono<Void> insertOne(DistributionAuditEvent e) {

        String sql = """
            INSERT INTO %s
            (load_row_id, data_key, service_name, state,
             publish_message, request_id, created_at)
            VALUES
            (:loadRowId, :dataKey, :serviceName, :state,
             :msg, :requestId, now())
            """.formatted(table);

        return db.sql(sql)
                .bind("loadRowId", e.loadRowId)
                .bind("dataKey", e.dataKey)
                .bind("serviceName", e.serviceName)
                .bind("state", e.state.name())
                .bind("msg", e.publishMessage)
                .bind("requestId", e.requestId)
                .fetch()
                .rowsUpdated()
                .then();
    }
}
